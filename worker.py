import os
import json
import requests
import tempfile
import time
from redis import Redis
from services.qr_processor import QRProcessor
from services.invoice_parser import InvoiceParser
from services.supabase_client import SupabaseService

# åˆå§‹åŒ–ç’°å¢ƒ
REDIS_URL = os.getenv("REDIS_URL", "redis://default:TjVSiTlscOkTrhUMXqyFmcHpPaFhDRsW@redis.railway.internal:6379")
redis_conn = Redis.from_url(REDIS_URL)
qr_processor = QRProcessor()
supabase_service = SupabaseService()

def process_task(task_data):
    """
    åŸ·è¡Œè¾¨è­˜ä»»å‹™ï¼šä¸‹è¼‰åœ–ç‰‡ -> è¾¨è­˜ -> å­˜å…¥ DB -> æ›´æ–° Redis
    """
    task_id = task_data.get("task_id")
    image_urls = task_data.get("image_urls", [])
    total = len(image_urls)
    all_results = []

    print(f"ğŸš€ [Worker] é–‹å§‹è™•ç†ä»»å‹™ {task_id}, å…± {total} å¼µåœ–ç‰‡")

    for idx, url in enumerate(image_urls):
        try:
            # 1. æ›´æ–°é€²åº¦ (0-100)
            progress = int(((idx + 1) / total) * 100)
            status_data = {
                "status": "processing",
                "current": idx + 1,
                "total": total,
                "progress": progress,
                "message": f"æ­£åœ¨è™•ç†ç¬¬ {idx+1}/{total} å¼µåœ–ç‰‡"
            }
            redis_conn.setex(f"task_status:{task_id}", 3600, json.dumps(status_data))

            # 2. ä¸‹è¼‰åœ–ç‰‡åˆ°æš«å­˜
            response = requests.get(url, timeout=30)
            if response.status_code != 200:
                continue

            with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
                tmp.write(response.content)
                temp_path = tmp.name

            # 3. åŸ·è¡Œè¾¨è­˜
            detections, decoded_texts = qr_processor.detect_and_decode(temp_path)
            # ä½¿ç”¨å¤šå¼µè¾¨è­˜çš„åˆ†çµ„é‚è¼¯
            groups = qr_processor.group_qrs_by_invoice(detections, decoded_texts)

            for g in groups:
                try:
                    m = InvoiceParser.parse_qr_code(g["left_data"])
                    m["source"] = "Queue"
                    # æª¢æŸ¥é‡è¤‡ä¸¦å„²å­˜
                    m["is_duplicate"] = supabase_service.check_duplicate(m["invoice_number"])
                    # ç›´æ¥ä½¿ç”¨å‚³å…¥çš„ Supabase URL å­˜å…¥ DB
                    supabase_service.save_invoice(m, url)
                    all_results.append(m)
                except Exception as parse_err:
                    print(f"è§£æå¤±æ•—: {parse_err}")
                    continue

            # æ¸…ç†æš«å­˜
            if os.path.exists(temp_path):
                os.unlink(temp_path)

        except Exception as e:
            print(f"âŒ [Worker] è™•ç†åœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {url}, Error: {e}")
            continue

    # 4. ä»»å‹™å®Œæˆ
    final_status = {
        "status": "completed",
        "current": total,
        "total": total,
        "progress": 100,
        "results": all_results
    }
    redis_conn.setex(f"task_status:{task_id}", 3600, json.dumps(final_status))
    print(f"âœ… [Worker] ä»»å‹™ {task_id} è™•ç†å®Œæˆï¼Œå…±è¾¨è­˜å‡º {len(all_results)} å¼µç™¼ç¥¨")

def main():
    print("ğŸ‘· Worker å·²å•Ÿå‹•ï¼Œæ­£åœ¨ç›£è½ä»»å‹™ä½‡åˆ—...")
    while True:
        # å¾ Redis åˆ—è¡¨å·¦å´å–å¾—ä»»å‹™ (Blocking POP)
        task_raw = redis_conn.blpop("invoice_tasks", timeout=30)
        if task_raw:
            try:
                task_data = json.loads(task_raw[1])
                process_task(task_data)
            except Exception as e:
                print(f"âŒ [Worker] åŸ·è¡Œä»»å‹™å¤±æ•—: {e}")
        else:
            # æ²’ä»»å‹™æ™‚å°ç¡ä¸€ä¸‹
            time.sleep(1)

if __name__ == "__main__":
    main()